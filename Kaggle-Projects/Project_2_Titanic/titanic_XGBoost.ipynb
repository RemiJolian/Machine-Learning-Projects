{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Project; using XGBoost model (by ChatGPT)\n",
    "Hereâ€™s a code example for solving the Titanic dataset problem using XGBoost, which includes data preprocessing, one-hot encoding, training the model, and saving the predictions to a submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "[0]\tvalidation-logloss:0.63834\n",
      "[1]\tvalidation-logloss:0.60491\n",
      "[2]\tvalidation-logloss:0.57781\n",
      "[3]\tvalidation-logloss:0.55565\n",
      "[4]\tvalidation-logloss:0.53516\n",
      "[5]\tvalidation-logloss:0.51880\n",
      "[6]\tvalidation-logloss:0.50420\n",
      "[7]\tvalidation-logloss:0.49192\n",
      "[8]\tvalidation-logloss:0.48274\n",
      "[9]\tvalidation-logloss:0.47385\n",
      "[10]\tvalidation-logloss:0.46556\n",
      "[11]\tvalidation-logloss:0.46030\n",
      "[12]\tvalidation-logloss:0.45513\n",
      "[13]\tvalidation-logloss:0.45100\n",
      "[14]\tvalidation-logloss:0.44747\n",
      "[15]\tvalidation-logloss:0.44494\n",
      "[16]\tvalidation-logloss:0.44298\n",
      "[17]\tvalidation-logloss:0.43988\n",
      "[18]\tvalidation-logloss:0.43805\n",
      "[19]\tvalidation-logloss:0.43706\n",
      "[20]\tvalidation-logloss:0.43554\n",
      "[21]\tvalidation-logloss:0.43444\n",
      "[22]\tvalidation-logloss:0.43458\n",
      "[23]\tvalidation-logloss:0.43221\n",
      "[24]\tvalidation-logloss:0.43107\n",
      "[25]\tvalidation-logloss:0.43054\n",
      "[26]\tvalidation-logloss:0.42998\n",
      "[27]\tvalidation-logloss:0.42903\n",
      "[28]\tvalidation-logloss:0.42890\n",
      "[29]\tvalidation-logloss:0.42838\n",
      "[30]\tvalidation-logloss:0.42960\n",
      "[31]\tvalidation-logloss:0.42820\n",
      "[32]\tvalidation-logloss:0.42749\n",
      "[33]\tvalidation-logloss:0.42721\n",
      "[34]\tvalidation-logloss:0.42604\n",
      "[35]\tvalidation-logloss:0.42527\n",
      "[36]\tvalidation-logloss:0.42248\n",
      "[37]\tvalidation-logloss:0.42263\n",
      "[38]\tvalidation-logloss:0.42201\n",
      "[39]\tvalidation-logloss:0.42031\n",
      "[40]\tvalidation-logloss:0.41972\n",
      "[41]\tvalidation-logloss:0.41996\n",
      "[42]\tvalidation-logloss:0.41943\n",
      "[43]\tvalidation-logloss:0.41875\n",
      "[44]\tvalidation-logloss:0.41840\n",
      "[45]\tvalidation-logloss:0.41761\n",
      "[46]\tvalidation-logloss:0.41671\n",
      "[47]\tvalidation-logloss:0.41609\n",
      "[48]\tvalidation-logloss:0.41541\n",
      "[49]\tvalidation-logloss:0.41462\n",
      "[50]\tvalidation-logloss:0.41396\n",
      "[51]\tvalidation-logloss:0.41492\n",
      "[52]\tvalidation-logloss:0.41498\n",
      "[53]\tvalidation-logloss:0.41378\n",
      "[54]\tvalidation-logloss:0.41352\n",
      "[55]\tvalidation-logloss:0.41319\n",
      "[56]\tvalidation-logloss:0.41256\n",
      "[57]\tvalidation-logloss:0.41221\n",
      "[58]\tvalidation-logloss:0.41164\n",
      "[59]\tvalidation-logloss:0.41158\n",
      "[60]\tvalidation-logloss:0.41250\n",
      "[61]\tvalidation-logloss:0.41266\n",
      "[62]\tvalidation-logloss:0.41279\n",
      "[63]\tvalidation-logloss:0.41278\n",
      "[64]\tvalidation-logloss:0.41312\n",
      "[65]\tvalidation-logloss:0.41296\n",
      "[66]\tvalidation-logloss:0.41273\n",
      "[67]\tvalidation-logloss:0.41234\n",
      "[68]\tvalidation-logloss:0.41150\n",
      "[69]\tvalidation-logloss:0.41143\n",
      "[70]\tvalidation-logloss:0.41207\n",
      "[71]\tvalidation-logloss:0.41167\n",
      "[72]\tvalidation-logloss:0.41158\n",
      "[73]\tvalidation-logloss:0.41169\n",
      "[74]\tvalidation-logloss:0.41175\n",
      "[75]\tvalidation-logloss:0.41098\n",
      "[76]\tvalidation-logloss:0.41091\n",
      "[77]\tvalidation-logloss:0.41103\n",
      "[78]\tvalidation-logloss:0.41126\n",
      "[79]\tvalidation-logloss:0.41187\n",
      "[80]\tvalidation-logloss:0.41170\n",
      "[81]\tvalidation-logloss:0.41164\n",
      "[82]\tvalidation-logloss:0.41140\n",
      "[83]\tvalidation-logloss:0.41182\n",
      "[84]\tvalidation-logloss:0.41196\n",
      "[85]\tvalidation-logloss:0.41196\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('./titanic_project_data/train.csv')\n",
    "test_data = pd.read_csv('./titanic_project_data/test.csv')\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "print(train_data.head())\n",
    "\n",
    "# Data cleaning\n",
    "# Fill missing Age values with the median age\n",
    "train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].median(), inplace=True)\n",
    "\n",
    "# Fill missing Embarked values with the most common port\n",
    "train_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Fill missing Fare values in the test set with the median fare\n",
    "test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)\n",
    "\n",
    "# Drop the Cabin column as it has many missing values\n",
    "train_data.drop(columns=['Cabin'], inplace=True)\n",
    "test_data.drop(columns=['Cabin'], inplace=True)\n",
    "\n",
    "# Drop the Ticket and Name columns as they may not contribute much to the prediction\n",
    "train_data.drop(columns=['Ticket', 'Name'], inplace=True)\n",
    "test_data.drop(columns=['Ticket', 'Name'], inplace=True)\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# Encode 'Sex' and 'Embarked' columns in the train dataset\n",
    "encoded_train = encoder.fit_transform(train_data[['Sex', 'Embarked']])\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(['Sex', 'Embarked']))\n",
    "\n",
    "# Encode 'Sex' and 'Embarked' columns in the test dataset\n",
    "encoded_test = encoder.transform(test_data[['Sex', 'Embarked']])\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(['Sex', 'Embarked']))\n",
    "\n",
    "# Concatenate encoded columns with original data\n",
    "train_data = pd.concat([train_data.drop(['Sex', 'Embarked'], axis=1), encoded_train_df], axis=1)\n",
    "test_data = pd.concat([test_data.drop(['Sex', 'Embarked'], axis=1), encoded_test_df], axis=1)\n",
    "\n",
    "# Split features and target variable\n",
    "X = train_data.drop(['Survived', 'PassengerId'], axis=1)\n",
    "y = train_data['Survived']\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert datasets into DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(test_data.drop('PassengerId', axis=1))\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100, evals=[(dval, 'validation')], early_stopping_rounds=10)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = bst.predict(dtest)\n",
    "predictions = np.where(predictions >= 0.5, 1, 0)  # Convert probabilities to binary outcome (0 or 1)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': predictions})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Your submission was successfully saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation for Reducing Runtime, Complexity, and Increasing Interpretability:\n",
    "\n",
    "1. **Reduce `max_depth` and `num_boost_round`:**\n",
    "   - Lowering the `max_depth` parameter reduces the complexity of individual trees, making the model less computationally expensive.\n",
    "   - Reducing `num_boost_round` can lower runtime, although it may slightly reduce accuracy.\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - Dropping less important features or using feature selection techniques can reduce model complexity. Dropping columns like 'PassengerId' or other less informative features can help.\n",
    "\n",
    "3. **Parameter Tuning:**\n",
    "   - Adjusting the `eta` (learning rate) can reduce the number of boosting rounds needed for convergence.\n",
    "   - Use regularization parameters (`alpha` and `lambda`) to prevent overfitting and make the model more interpretable.\n",
    "\n",
    "4. **Use a simpler model:**\n",
    "   - For increased interpretability, consider using logistic regression or decision trees instead of XGBoost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If youâ€™re not using `train_test_split`, you can directly train the model using the entire `train_data` for training and then use `test_data` for predictions. Hereâ€™s the revised code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('./titanic_project_data/train.csv')\n",
    "test_data = pd.read_csv('./titanic_project_data/test.csv')\n",
    "\n",
    "# Data cleaning\n",
    "# Fill missing Age values with the median age\n",
    "train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].median(), inplace=True)\n",
    "\n",
    "# Fill missing Embarked values with the most common port\n",
    "train_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Fill missing Fare values in the test set with the median fare\n",
    "test_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)\n",
    "\n",
    "# Drop the Cabin column as it has many missing values\n",
    "train_data.drop(columns=['Cabin'], inplace=True)\n",
    "test_data.drop(columns=['Cabin'], inplace=True)\n",
    "\n",
    "# Drop the Ticket and Name columns as they may not contribute much to the prediction\n",
    "train_data.drop(columns=['Ticket', 'Name'], inplace=True)\n",
    "test_data.drop(columns=['Ticket', 'Name'], inplace=True)\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# Encode 'Sex' and 'Embarked' columns in the train dataset\n",
    "encoded_train = encoder.fit_transform(train_data[['Sex', 'Embarked']])\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(['Sex', 'Embarked']))\n",
    "\n",
    "# Encode 'Sex' and 'Embarked' columns in the test dataset\n",
    "encoded_test = encoder.transform(test_data[['Sex', 'Embarked']])\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(['Sex', 'Embarked']))\n",
    "\n",
    "# Concatenate encoded columns with original data\n",
    "train_data = pd.concat([train_data.drop(['Sex', 'Embarked'], axis=1), encoded_train_df], axis=1)\n",
    "test_data = pd.concat([test_data.drop(['Sex', 'Embarked'], axis=1), encoded_test_df], axis=1)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_data.drop(['Survived', 'PassengerId'], axis=1)\n",
    "y_train = train_data['Survived']\n",
    "\n",
    "# Convert training data to DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(test_data.drop('PassengerId', axis=1))\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = bst.predict(dtest)\n",
    "predictions = np.where(predictions >= 0.5, 1, 0)  # Convert probabilities to binary outcome (0 or 1)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': predictions})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Key Changes:\n",
    "1. **No Validation Split:** The `train_test_split` step was removed since you will use the entire training data (`train_data`) for training and directly make predictions on the provided `test_data`.\n",
    "2. **Model Training:** The XGBoost model is trained on all of the provided training data without any separation for validation.\n",
    "3. **Output Prediction:** Predictions are made directly on the test data and saved as `submission.csv`.\n",
    "\n",
    "### Impact on Runtime, Complexity, and Interpretability:\n",
    "- **Decrease Runtime and Complexity:** Reduce the `max_depth`, lower `num_boost_round`, or tune other hyperparameters like `eta` for faster training. Feature selection could also reduce runtime by using fewer features.\n",
    "- **Increase Interpretability:** Consider using fewer features, a simpler model like a decision tree, or applying SHAP values for feature importance explanation in XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
