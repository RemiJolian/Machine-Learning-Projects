{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01913c65",
   "metadata": {},
   "source": [
    "### Kaggle suggest this notebook to get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad322a",
   "metadata": {
    "papermill": {
     "duration": 0.009041,
     "end_time": "2023-05-18T10:30:46.149968",
     "exception": false,
     "start_time": "2023-05-18T10:30:46.140927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Titanic competition with TensorFlow Decision Forests\n",
    "\n",
    "This notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition. \n",
    "\n",
    "This notebook shows:\n",
    "\n",
    "1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\n",
    "1. How to train a Gradient Boosted Trees (GBT) with default parameters\n",
    "1. How to train a GBT with improved default parameters\n",
    "1. How to tune the parameters of a GBTs\n",
    "1. How to train and ensemble many GBTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b37618",
   "metadata": {
    "papermill": {
     "duration": 0.007644,
     "end_time": "2023-05-18T10:30:46.165684",
     "exception": false,
     "start_time": "2023-05-18T10:30:46.158040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58766b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:30:46.184015Z",
     "iopub.status.busy": "2023-05-18T10:30:46.183207Z",
     "iopub.status.idle": "2023-05-18T10:30:56.048327Z",
     "shell.execute_reply": "2023-05-18T10:30:56.046442Z"
    },
    "papermill": {
     "duration": 9.877832,
     "end_time": "2023-05-18T10:30:56.051537",
     "exception": false,
     "start_time": "2023-05-18T10:30:46.173705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow Decision Forests 1.8.1 is compatible with the following TensorFlow Versions: ['2.15.0']. However, TensorFlow 2.16.1 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n",
      "WARNING:root:Failure to load the inference.so custom c++ tensorflow ops. This error is likely caused the version of TensorFlow and TensorFlow Decision Forests are not compatible. Full error:c:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "c:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfdf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound TF-DF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfdf\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\__init__.py:64\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_version\n\u001b[0;32m     62\u001b[0m check_version\u001b[38;5;241m.\u001b[39mcheck_version(__version__, compatible_tf_versions)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_tree\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\__init__.py:53\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Decision Forest in a Keras Model.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mUsage example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrappers\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Utility classes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core.py:62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tuner \u001b[38;5;28;01mas\u001b[39;00m tuner_lib\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cc_logging\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core \u001b[38;5;28;01mas\u001b[39;00m tf_core\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core_inference.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_inference \u001b[38;5;28;01mas\u001b[39;00m tf_core\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m tf_op\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_learner_pb2\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultitasker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multitasker_pb2\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\api.py:179\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inspector \u001b[38;5;28;01mas\u001b[39;00m inspector_lib\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf1_compatibility\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_spec_pb2\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myggdrasil_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abstract_model_pb2\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 Google LLC.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_decision_forests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_dynamic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Importing all the symbols.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m   ops \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_op_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_to_datafile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference.so\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     23\u001b[0m   check_version\u001b[38;5;241m.\u001b[39minfo_fail_to_load_custom_op(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference.so\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\load_library.py:54\u001b[0m, in \u001b[0;36mload_op_library\u001b[1;34m(library_filename)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_op_library\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_op_library\u001b[39m(library_filename):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a TensorFlow plugin, containing custom ops and kernels.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m  Pass \"library_filename\" to a platform-specific mechanism for dynamically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RuntimeError: when unable to load the library or get the python wrappers.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m   lib_handle \u001b[38;5;241m=\u001b[39m \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     wrappers \u001b[38;5;241m=\u001b[39m _pywrap_python_op_gen\u001b[38;5;241m.\u001b[39mGetPythonWrappers(\n\u001b[0;32m     57\u001b[0m         py_tf\u001b[38;5;241m.\u001b[39mTF_GetOpList(lib_handle))\n",
      "\u001b[1;31mNotFoundError\u001b[0m: c:\\Users\\Laptop Market - LTM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "print(f\"Found TF-DF {tfdf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b86c6e",
   "metadata": {
    "papermill": {
     "duration": 0.008393,
     "end_time": "2023-05-18T10:30:56.068502",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.060109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bd1480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:30:56.087356Z",
     "iopub.status.busy": "2023-05-18T10:30:56.086566Z",
     "iopub.status.idle": "2023-05-18T10:30:56.153903Z",
     "shell.execute_reply": "2023-05-18T10:30:56.152399Z"
    },
    "papermill": {
     "duration": 0.080398,
     "end_time": "2023-05-18T10:30:56.157107",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.076709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./titanic_project data/train.csv\")\n",
    "serving_df = pd.read_csv(\"./titanic_project data/test.csv\")\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d80b8",
   "metadata": {
    "papermill": {
     "duration": 0.008374,
     "end_time": "2023-05-18T10:30:56.174348",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.165974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare dataset\n",
    "\n",
    "We will apply the following transformations on the dataset.\n",
    "\n",
    "1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n",
    "2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a35a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:30:56.194311Z",
     "iopub.status.busy": "2023-05-18T10:30:56.193387Z",
     "iopub.status.idle": "2023-05-18T10:30:56.235430Z",
     "shell.execute_reply": "2023-05-18T10:30:56.234013Z"
    },
    "papermill": {
     "duration": 0.055455,
     "end_time": "2023-05-18T10:30:56.238539",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.183084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicket_item\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicket\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(ticket_item)                     \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 21\u001b[0m preprocessed_train_df \u001b[38;5;241m=\u001b[39m preprocess(\u001b[43mtrain_df\u001b[49m)\n\u001b[0;32m     22\u001b[0m preprocessed_serving_df \u001b[38;5;241m=\u001b[39m preprocess(serving_df)\n\u001b[0;32m     24\u001b[0m preprocessed_train_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    def normalize_name(x):\n",
    "        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n",
    "    \n",
    "    def ticket_number(x):\n",
    "        return x.split(\" \")[-1]\n",
    "        \n",
    "    def ticket_item(x):\n",
    "        items = x.split(\" \")\n",
    "        if len(items) == 1:\n",
    "            return \"NONE\"\n",
    "        return \"_\".join(items[0:-1])\n",
    "    \n",
    "    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n",
    "    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n",
    "    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n",
    "    return df\n",
    "    \n",
    "preprocessed_train_df = preprocess(train_df)\n",
    "preprocessed_serving_df = preprocess(serving_df)\n",
    "\n",
    "preprocessed_train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25350e15",
   "metadata": {
    "papermill": {
     "duration": 0.008741,
     "end_time": "2023-05-18T10:30:56.256397",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.247656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's keep the list of the input features of the model. Notably, we don't want to train our model on the \"PassengerId\" and \"Ticket\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19722265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:30:56.276567Z",
     "iopub.status.busy": "2023-05-18T10:30:56.276120Z",
     "iopub.status.idle": "2023-05-18T10:30:56.283712Z",
     "shell.execute_reply": "2023-05-18T10:30:56.282042Z"
    },
    "papermill": {
     "duration": 0.0219,
     "end_time": "2023-05-18T10:30:56.287214",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.265314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features: ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', 'Ticket_number', 'Ticket_item']\n"
     ]
    }
   ],
   "source": [
    "input_features = list(preprocessed_train_df.columns)\n",
    "input_features.remove(\"Ticket\")\n",
    "input_features.remove(\"PassengerId\")\n",
    "input_features.remove(\"Survived\")\n",
    "#input_features.remove(\"Ticket_number\")\n",
    "\n",
    "print(f\"Input features: {input_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f8550",
   "metadata": {
    "papermill": {
     "duration": 0.00868,
     "end_time": "2023-05-18T10:30:56.305032",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.296352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert Pandas dataset to TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b109d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:30:56.327328Z",
     "iopub.status.busy": "2023-05-18T10:30:56.326875Z",
     "iopub.status.idle": "2023-05-18T10:30:56.701743Z",
     "shell.execute_reply": "2023-05-18T10:30:56.700236Z"
    },
    "papermill": {
     "duration": 0.390876,
     "end_time": "2023-05-18T10:30:56.705086",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.314210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_names(features, labels=None):\n",
    "    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n",
    "    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n",
    "    return features, labels\n",
    "\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\n",
    "serving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa067c2b",
   "metadata": {
    "papermill": {
     "duration": 0.008633,
     "end_time": "2023-05-18T10:30:56.722976",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.714343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train model with default parameters\n",
    "\n",
    "### Train model\n",
    "\n",
    "First, we are training a GradientBoostedTreesModel model with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1d87d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:30:56.743940Z",
     "iopub.status.busy": "2023-05-18T10:30:56.742522Z",
     "iopub.status.idle": "2023-05-18T10:31:09.175345Z",
     "shell.execute_reply": "2023-05-18T10:31:09.174038Z"
    },
    "papermill": {
     "duration": 12.446474,
     "end_time": "2023-05-18T10:31:09.178415",
     "exception": false,
     "start_time": "2023-05-18T10:30:56.731941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2023-05-18T10:31:05.469776904+00:00 kernel.cc:1214] Loading model from path /tmp/tmpxl2c60xw/model/ with prefix f38ff16f536e4497\n",
      "[INFO 2023-05-18T10:31:05.47954519+00:00 abstract_model.cc:1311] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n",
      "[INFO 2023-05-18T10:31:05.479865457+00:00 kernel.cc:1046] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x78705a4f94d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Accuracy: 0.8260869383811951 Loss:0.8608942627906799\n"
     ]
    }
   ],
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(\n",
    "    verbose=0, # Very few logs\n",
    "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "    exclude_non_specified_features=True, # Only use the features in \"features\"\n",
    "    random_seed=1234,\n",
    ")\n",
    "model.fit(train_ds)\n",
    "\n",
    "self_evaluation = model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b947981",
   "metadata": {
    "papermill": {
     "duration": 0.009801,
     "end_time": "2023-05-18T10:31:09.197829",
     "exception": false,
     "start_time": "2023-05-18T10:31:09.188028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train model with improved default parameters\n",
    "\n",
    "Now you'll use some specific parameters when creating the GBT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9c8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:31:09.219199Z",
     "iopub.status.busy": "2023-05-18T10:31:09.218753Z",
     "iopub.status.idle": "2023-05-18T10:31:10.588143Z",
     "shell.execute_reply": "2023-05-18T10:31:10.586774Z"
    },
    "papermill": {
     "duration": 1.383354,
     "end_time": "2023-05-18T10:31:10.591287",
     "exception": false,
     "start_time": "2023-05-18T10:31:09.207933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2023-05-18T10:31:10.217810247+00:00 kernel.cc:1214] Loading model from path /tmp/tmp73d7qv4h/model/ with prefix ce08288098554ec5\n",
      "[INFO 2023-05-18T10:31:10.227982178+00:00 decision_forest.cc:661] Model loaded with 33 root(s), 1823 node(s), and 10 input feature(s).\n",
      "[INFO 2023-05-18T10:31:10.228265252+00:00 kernel.cc:1046] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.760869562625885 Loss:1.0154211521148682\n"
     ]
    }
   ],
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(\n",
    "    verbose=0, # Very few logs\n",
    "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "    exclude_non_specified_features=True, # Only use the features in \"features\"\n",
    "    \n",
    "    #num_trees=2000,\n",
    "    \n",
    "    # Only for GBT.\n",
    "    # A bit slower, but great to understand the model.\n",
    "    # compute_permutation_variable_importance=True,\n",
    "    \n",
    "    # Change the default hyper-parameters\n",
    "    # hyperparameter_template=\"benchmark_rank1@v1\",\n",
    "    \n",
    "    #num_trees=1000,\n",
    "    #tuner=tuner\n",
    "    \n",
    "    min_examples=1,\n",
    "    categorical_algorithm=\"RANDOM\",\n",
    "    #max_depth=4,\n",
    "    shrinkage=0.05,\n",
    "    #num_candidate_attributes_ratio=0.2,\n",
    "    split_axis=\"SPARSE_OBLIQUE\",\n",
    "    sparse_oblique_normalization=\"MIN_MAX\",\n",
    "    sparse_oblique_num_projections_exponent=2.0,\n",
    "    num_trees=2000,\n",
    "    #validation_ratio=0.0,\n",
    "    random_seed=1234,\n",
    "    \n",
    ")\n",
    "model.fit(train_ds)\n",
    "\n",
    "self_evaluation = model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d004f8",
   "metadata": {
    "papermill": {
     "duration": 0.009209,
     "end_time": "2023-05-18T10:31:10.610105",
     "exception": false,
     "start_time": "2023-05-18T10:31:10.600896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's look at the model and you can also notice the information about variable importance that the model figured out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6418f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:31:10.632417Z",
     "iopub.status.busy": "2023-05-18T10:31:10.631850Z",
     "iopub.status.idle": "2023-05-18T10:31:10.649979Z",
     "shell.execute_reply": "2023-05-18T10:31:10.649004Z"
    },
    "papermill": {
     "duration": 0.038389,
     "end_time": "2023-05-18T10:31:10.658474",
     "exception": false,
     "start_time": "2023-05-18T10:31:10.620085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gradient_boosted_trees_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 1\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Type: \"GRADIENT_BOOSTED_TREES\"\n",
      "Task: CLASSIFICATION\n",
      "Label: \"__LABEL\"\n",
      "\n",
      "Input Features (11):\n",
      "\tAge\n",
      "\tCabin\n",
      "\tEmbarked\n",
      "\tFare\n",
      "\tName\n",
      "\tParch\n",
      "\tPclass\n",
      "\tSex\n",
      "\tSibSp\n",
      "\tTicket_item\n",
      "\tTicket_number\n",
      "\n",
      "No weights\n",
      "\n",
      "Variable Importance: INV_MEAN_MIN_DEPTH:\n",
      "    1.           \"Sex\"  0.576632 ################\n",
      "    2.           \"Age\"  0.364297 #######\n",
      "    3.          \"Fare\"  0.278839 ####\n",
      "    4.          \"Name\"  0.208548 #\n",
      "    5. \"Ticket_number\"  0.180792 \n",
      "    6.        \"Pclass\"  0.176962 \n",
      "    7.         \"Parch\"  0.176659 \n",
      "    8.   \"Ticket_item\"  0.175540 \n",
      "    9.      \"Embarked\"  0.172339 \n",
      "   10.         \"SibSp\"  0.170442 \n",
      "\n",
      "Variable Importance: NUM_AS_ROOT:\n",
      "    1.  \"Sex\" 28.000000 ################\n",
      "    2. \"Name\"  5.000000 \n",
      "\n",
      "Variable Importance: NUM_NODES:\n",
      "    1.           \"Age\" 406.000000 ################\n",
      "    2.          \"Fare\" 290.000000 ###########\n",
      "    3.          \"Name\" 44.000000 #\n",
      "    4.   \"Ticket_item\" 42.000000 #\n",
      "    5.           \"Sex\" 31.000000 #\n",
      "    6.         \"Parch\" 28.000000 \n",
      "    7. \"Ticket_number\" 22.000000 \n",
      "    8.        \"Pclass\" 15.000000 \n",
      "    9.      \"Embarked\" 12.000000 \n",
      "   10.         \"SibSp\"  5.000000 \n",
      "\n",
      "Variable Importance: SUM_SCORE:\n",
      "    1.           \"Sex\" 460.497828 ################\n",
      "    2.           \"Age\" 355.963333 ############\n",
      "    3.          \"Fare\" 292.870316 ##########\n",
      "    4.          \"Name\" 108.548952 ###\n",
      "    5.        \"Pclass\" 28.132254 \n",
      "    6.   \"Ticket_item\" 23.818676 \n",
      "    7. \"Ticket_number\" 23.772288 \n",
      "    8.         \"Parch\" 19.303155 \n",
      "    9.      \"Embarked\"  8.155722 \n",
      "   10.         \"SibSp\"  0.015225 \n",
      "\n",
      "\n",
      "\n",
      "Loss: BINOMIAL_LOG_LIKELIHOOD\n",
      "Validation loss value: 1.01542\n",
      "Number of trees per iteration: 1\n",
      "Node format: NOT_SET\n",
      "Number of trees: 33\n",
      "Total number of nodes: 1823\n",
      "\n",
      "Number of nodes by tree:\n",
      "Count: 33 Average: 55.2424 StdDev: 5.13473\n",
      "Min: 39 Max: 63 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 39, 40) 1   3.03%   3.03% #\n",
      "[ 40, 41) 0   0.00%   3.03%\n",
      "[ 41, 42) 0   0.00%   3.03%\n",
      "[ 42, 44) 0   0.00%   3.03%\n",
      "[ 44, 45) 0   0.00%   3.03%\n",
      "[ 45, 46) 0   0.00%   3.03%\n",
      "[ 46, 47) 0   0.00%   3.03%\n",
      "[ 47, 49) 2   6.06%   9.09% ###\n",
      "[ 49, 50) 2   6.06%  15.15% ###\n",
      "[ 50, 51) 0   0.00%  15.15%\n",
      "[ 51, 52) 2   6.06%  21.21% ###\n",
      "[ 52, 54) 5  15.15%  36.36% #######\n",
      "[ 54, 55) 0   0.00%  36.36%\n",
      "[ 55, 56) 5  15.15%  51.52% #######\n",
      "[ 56, 57) 0   0.00%  51.52%\n",
      "[ 57, 59) 4  12.12%  63.64% ######\n",
      "[ 59, 60) 7  21.21%  84.85% ##########\n",
      "[ 60, 61) 0   0.00%  84.85%\n",
      "[ 61, 62) 3   9.09%  93.94% ####\n",
      "[ 62, 63] 2   6.06% 100.00% ###\n",
      "\n",
      "Depth by leafs:\n",
      "Count: 928 Average: 4.8847 StdDev: 0.380934\n",
      "Min: 2 Max: 5 Ignored: 0\n",
      "----------------------------------------------\n",
      "[ 2, 3)   1   0.11%   0.11%\n",
      "[ 3, 4)  17   1.83%   1.94%\n",
      "[ 4, 5)  70   7.54%   9.48% #\n",
      "[ 5, 5] 840  90.52% 100.00% ##########\n",
      "\n",
      "Number of training obs by leaf:\n",
      "Count: 928 Average: 28.4127 StdDev: 70.8313\n",
      "Min: 1 Max: 438 Ignored: 0\n",
      "----------------------------------------------\n",
      "[   1,  22) 731  78.77%  78.77% ##########\n",
      "[  22,  44)  74   7.97%  86.75% #\n",
      "[  44,  66)  37   3.99%  90.73% #\n",
      "[  66,  88)   3   0.32%  91.06%\n",
      "[  88, 110)   9   0.97%  92.03%\n",
      "[ 110, 132)   8   0.86%  92.89%\n",
      "[ 132, 154)  18   1.94%  94.83%\n",
      "[ 154, 176)   8   0.86%  95.69%\n",
      "[ 176, 198)   6   0.65%  96.34%\n",
      "[ 198, 220)   2   0.22%  96.55%\n",
      "[ 220, 241)   2   0.22%  96.77%\n",
      "[ 241, 263)   1   0.11%  96.88%\n",
      "[ 263, 285)   2   0.22%  97.09%\n",
      "[ 285, 307)   5   0.54%  97.63%\n",
      "[ 307, 329)   1   0.11%  97.74%\n",
      "[ 329, 351)   2   0.22%  97.95%\n",
      "[ 351, 373)   6   0.65%  98.60%\n",
      "[ 373, 395)   6   0.65%  99.25%\n",
      "[ 395, 417)   2   0.22%  99.46%\n",
      "[ 417, 438]   5   0.54% 100.00%\n",
      "\n",
      "Attribute in nodes:\n",
      "\t406 : Age [NUMERICAL]\n",
      "\t290 : Fare [NUMERICAL]\n",
      "\t44 : Name [CATEGORICAL_SET]\n",
      "\t42 : Ticket_item [CATEGORICAL]\n",
      "\t31 : Sex [CATEGORICAL]\n",
      "\t28 : Parch [NUMERICAL]\n",
      "\t22 : Ticket_number [CATEGORICAL]\n",
      "\t15 : Pclass [NUMERICAL]\n",
      "\t12 : Embarked [CATEGORICAL]\n",
      "\t5 : SibSp [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 0:\n",
      "\t28 : Sex [CATEGORICAL]\n",
      "\t5 : Name [CATEGORICAL_SET]\n",
      "\n",
      "Attribute in nodes with depth <= 1:\n",
      "\t39 : Age [NUMERICAL]\n",
      "\t28 : Sex [CATEGORICAL]\n",
      "\t21 : Fare [NUMERICAL]\n",
      "\t5 : Name [CATEGORICAL_SET]\n",
      "\t3 : Pclass [NUMERICAL]\n",
      "\t2 : Ticket_number [CATEGORICAL]\n",
      "\t1 : Parch [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 2:\n",
      "\t102 : Age [NUMERICAL]\n",
      "\t65 : Fare [NUMERICAL]\n",
      "\t28 : Sex [CATEGORICAL]\n",
      "\t15 : Name [CATEGORICAL_SET]\n",
      "\t7 : Ticket_number [CATEGORICAL]\n",
      "\t5 : Pclass [NUMERICAL]\n",
      "\t4 : Parch [NUMERICAL]\n",
      "\t2 : Ticket_item [CATEGORICAL]\n",
      "\t2 : Embarked [CATEGORICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 3:\n",
      "\t206 : Age [NUMERICAL]\n",
      "\t156 : Fare [NUMERICAL]\n",
      "\t33 : Name [CATEGORICAL_SET]\n",
      "\t29 : Sex [CATEGORICAL]\n",
      "\t19 : Ticket_number [CATEGORICAL]\n",
      "\t11 : Ticket_item [CATEGORICAL]\n",
      "\t11 : Parch [NUMERICAL]\n",
      "\t7 : Pclass [NUMERICAL]\n",
      "\t3 : Embarked [CATEGORICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 5:\n",
      "\t406 : Age [NUMERICAL]\n",
      "\t290 : Fare [NUMERICAL]\n",
      "\t44 : Name [CATEGORICAL_SET]\n",
      "\t42 : Ticket_item [CATEGORICAL]\n",
      "\t31 : Sex [CATEGORICAL]\n",
      "\t28 : Parch [NUMERICAL]\n",
      "\t22 : Ticket_number [CATEGORICAL]\n",
      "\t15 : Pclass [NUMERICAL]\n",
      "\t12 : Embarked [CATEGORICAL]\n",
      "\t5 : SibSp [NUMERICAL]\n",
      "\n",
      "Condition type in nodes:\n",
      "\t744 : ObliqueCondition\n",
      "\t122 : ContainsBitmapCondition\n",
      "\t29 : ContainsCondition\n",
      "Condition type in nodes with depth <= 0:\n",
      "\t31 : ContainsBitmapCondition\n",
      "\t2 : ContainsCondition\n",
      "Condition type in nodes with depth <= 1:\n",
      "\t64 : ObliqueCondition\n",
      "\t33 : ContainsBitmapCondition\n",
      "\t2 : ContainsCondition\n",
      "Condition type in nodes with depth <= 2:\n",
      "\t176 : ObliqueCondition\n",
      "\t51 : ContainsBitmapCondition\n",
      "\t3 : ContainsCondition\n",
      "Condition type in nodes with depth <= 3:\n",
      "\t380 : ObliqueCondition\n",
      "\t77 : ContainsBitmapCondition\n",
      "\t18 : ContainsCondition\n",
      "Condition type in nodes with depth <= 5:\n",
      "\t744 : ObliqueCondition\n",
      "\t122 : ContainsBitmapCondition\n",
      "\t29 : ContainsCondition\n",
      "\n",
      "Training logs:\n",
      "Number of iteration to final model: 33\n",
      "\tIter:1 train-loss:1.266350 valid-loss:1.360049  train-accuracy:0.624531 valid-accuracy:0.543478\n",
      "\tIter:2 train-loss:1.213702 valid-loss:1.321897  train-accuracy:0.624531 valid-accuracy:0.543478\n",
      "\tIter:3 train-loss:1.165783 valid-loss:1.286817  train-accuracy:0.624531 valid-accuracy:0.543478\n",
      "\tIter:4 train-loss:1.122469 valid-loss:1.256133  train-accuracy:0.624531 valid-accuracy:0.543478\n",
      "\tIter:5 train-loss:1.081461 valid-loss:1.229342  train-accuracy:0.808511 valid-accuracy:0.771739\n",
      "\tIter:6 train-loss:1.045305 valid-loss:1.204601  train-accuracy:0.826033 valid-accuracy:0.728261\n",
      "\tIter:16 train-loss:0.794952 valid-loss:1.058568  train-accuracy:0.914894 valid-accuracy:0.771739\n",
      "\tIter:26 train-loss:0.646146 valid-loss:1.021539  train-accuracy:0.926158 valid-accuracy:0.793478\n",
      "\tIter:36 train-loss:0.558627 valid-loss:1.023663  train-accuracy:0.929912 valid-accuracy:0.771739\n",
      "\tIter:46 train-loss:0.493899 valid-loss:1.025164  train-accuracy:0.931164 valid-accuracy:0.760870\n",
      "\tIter:56 train-loss:0.451528 valid-loss:1.032880  train-accuracy:0.938673 valid-accuracy:0.771739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e15fa9",
   "metadata": {
    "papermill": {
     "duration": 0.010729,
     "end_time": "2023-05-18T10:31:10.681492",
     "exception": false,
     "start_time": "2023-05-18T10:31:10.670763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239df35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:31:10.706310Z",
     "iopub.status.busy": "2023-05-18T10:31:10.705862Z",
     "iopub.status.idle": "2023-05-18T10:31:12.005605Z",
     "shell.execute_reply": "2023-05-18T10:31:12.004033Z"
    },
    "papermill": {
     "duration": 1.316069,
     "end_time": "2023-05-18T10:31:12.008967",
     "exception": false,
     "start_time": "2023-05-18T10:31:10.692898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission exported to /kaggle/working/submission.csv\n",
      "PassengerId,Survived\r\n",
      "892,0\r\n",
      "893,0\r\n",
      "894,0\r\n",
      "895,0\r\n",
      "896,0\r\n",
      "897,0\r\n",
      "898,0\r\n",
      "899,0\r\n",
      "900,1\r\n"
     ]
    }
   ],
   "source": [
    "def prediction_to_kaggle_format(model, threshold=0.5):\n",
    "    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n",
    "    return pd.DataFrame({\n",
    "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
    "        \"Survived\": (proba_survive >= threshold).astype(int)\n",
    "    })\n",
    "\n",
    "def make_submission(kaggle_predictions):\n",
    "    path=\"/kaggle/working/submission.csv\"\n",
    "    kaggle_predictions.to_csv(path, index=False)\n",
    "    print(f\"Submission exported to {path}\")\n",
    "    \n",
    "kaggle_predictions = prediction_to_kaggle_format(model)\n",
    "make_submission(kaggle_predictions)\n",
    "!head /kaggle/working/submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9081865",
   "metadata": {
    "papermill": {
     "duration": 0.011739,
     "end_time": "2023-05-18T10:31:12.032772",
     "exception": false,
     "start_time": "2023-05-18T10:31:12.021033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training a model with hyperparameter tunning\n",
    "\n",
    "Hyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693406c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:31:12.057095Z",
     "iopub.status.busy": "2023-05-18T10:31:12.056551Z",
     "iopub.status.idle": "2023-05-18T10:33:21.245537Z",
     "shell.execute_reply": "2023-05-18T10:33:21.243670Z"
    },
    "papermill": {
     "duration": 129.205111,
     "end_time": "2023-05-18T10:33:21.248657",
     "exception": false,
     "start_time": "2023-05-18T10:31:12.043546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmpf3gqf8yh as temporary training directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2023-05-18T10:33:20.758894639+00:00 kernel.cc:1214] Loading model from path /tmp/tmpf3gqf8yh/model/ with prefix 1800e47d98cd4401\n",
      "[INFO 2023-05-18T10:33:20.773899277+00:00 decision_forest.cc:661] Model loaded with 19 root(s), 589 node(s), and 12 input feature(s).\n",
      "[INFO 2023-05-18T10:33:20.773949099+00:00 abstract_model.cc:1311] Engine \"GradientBoostedTreesGeneric\" built\n",
      "[INFO 2023-05-18T10:33:20.773977709+00:00 kernel.cc:1046] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9178082346916199 Loss:0.6503586769104004\n"
     ]
    }
   ],
   "source": [
    "tuner = tfdf.tuner.RandomSearch(num_trials=1000)\n",
    "tuner.choice(\"min_examples\", [2, 5, 7, 10])\n",
    "tuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n",
    "\n",
    "local_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\n",
    "local_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n",
    "\n",
    "global_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\n",
    "global_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n",
    "\n",
    "#tuner.choice(\"use_hessian_gain\", [True, False])\n",
    "tuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\n",
    "tuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n",
    "\n",
    "\n",
    "tuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n",
    "oblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\n",
    "oblique_space.choice(\"sparse_oblique_normalization\",\n",
    "                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\n",
    "oblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\n",
    "oblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n",
    "\n",
    "# Tune the model. Notice the `tuner=tuner`.\n",
    "tuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\n",
    "tuned_model.fit(train_ds, verbose=0)\n",
    "\n",
    "tuned_self_evaluation = tuned_model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90906558",
   "metadata": {
    "papermill": {
     "duration": 0.011276,
     "end_time": "2023-05-18T10:33:21.271148",
     "exception": false,
     "start_time": "2023-05-18T10:33:21.259872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\n",
    "\n",
    "This is the main idea behing hyperparameter tuning.\n",
    "\n",
    "For more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507d9c3",
   "metadata": {
    "papermill": {
     "duration": 0.010908,
     "end_time": "2023-05-18T10:33:21.293210",
     "exception": false,
     "start_time": "2023-05-18T10:33:21.282302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Making an ensemble\n",
    "\n",
    "Here you'll create 100 models with different seeds and combine their results\n",
    "\n",
    "This approach removes a little bit the random aspects related to creating ML models\n",
    "\n",
    "In the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eafd73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T10:33:21.317693Z",
     "iopub.status.busy": "2023-05-18T10:33:21.317026Z",
     "iopub.status.idle": "2023-05-18T10:35:53.444299Z",
     "shell.execute_reply": "2023-05-18T10:35:53.443138Z"
    },
    "papermill": {
     "duration": 152.142847,
     "end_time": "2023-05-18T10:35:53.447145",
     "exception": false,
     "start_time": "2023-05-18T10:33:21.304298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = None\n",
    "num_predictions = 0\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"i:{i}\")\n",
    "    # Possible models: GradientBoostedTreesModel or RandomForestModel\n",
    "    model = tfdf.keras.GradientBoostedTreesModel(\n",
    "        verbose=0, # Very few logs\n",
    "        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "        exclude_non_specified_features=True, # Only use the features in \"features\"\n",
    "\n",
    "        #min_examples=1,\n",
    "        #categorical_algorithm=\"RANDOM\",\n",
    "        ##max_depth=4,\n",
    "        #shrinkage=0.05,\n",
    "        ##num_candidate_attributes_ratio=0.2,\n",
    "        #split_axis=\"SPARSE_OBLIQUE\",\n",
    "        #sparse_oblique_normalization=\"MIN_MAX\",\n",
    "        #sparse_oblique_num_projections_exponent=2.0,\n",
    "        #num_trees=2000,\n",
    "        ##validation_ratio=0.0,\n",
    "        random_seed=i,\n",
    "        honest=True,\n",
    "    )\n",
    "    model.fit(train_ds)\n",
    "    \n",
    "    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n",
    "    if predictions is None:\n",
    "        predictions = sub_predictions\n",
    "    else:\n",
    "        predictions += sub_predictions\n",
    "    num_predictions += 1\n",
    "\n",
    "predictions/=num_predictions\n",
    "\n",
    "kaggle_predictions = pd.DataFrame({\n",
    "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
    "        \"Survived\": (predictions >= 0.5).astype(int)\n",
    "    })\n",
    "\n",
    "make_submission(kaggle_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700c5851",
   "metadata": {
    "papermill": {
     "duration": 0.028523,
     "end_time": "2023-05-18T10:35:53.504450",
     "exception": false,
     "start_time": "2023-05-18T10:35:53.475927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is next\n",
    "\n",
    "If you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 324.686677,
   "end_time": "2023-05-18T10:35:56.847460",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-18T10:30:32.160783",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
